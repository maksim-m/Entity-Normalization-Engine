{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from livelossplot import PlotLosses\n",
    "from torch import nn\n",
    "from torch.utils.data import SubsetRandomSampler, DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from classification.ClassificationDataset import ClassificationDataset\n",
    "from classification.model.SentenceTransformerAndClassifier import SentenceTransformerAndClassifier\n",
    "from utils import PROJECT_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 64\n",
    "BATCH_SIZE = 512\n",
    "SHUFFLE = True\n",
    "SEED = 42\n",
    "VALIDATION_SPLIT = 0.05\n",
    "EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Using device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_dataloaders(dataset: ClassificationDataset, validation_split: float):\n",
    "    dataset_size = len(dataset)\n",
    "    print(\"Dataset size: \", dataset_size)\n",
    "    print(dataset.index2label)\n",
    "\n",
    "    indices = list(range(dataset_size))\n",
    "    split = int(np.floor(validation_split * dataset_size))\n",
    "    if SHUFFLE:\n",
    "        np.random.seed(SEED)\n",
    "        np.random.shuffle(indices)\n",
    "    train_indices, val_indices = indices[split:], indices[:split]\n",
    "    print(\"Train size: {}, Val size: {}\".format(len(train_indices), len(val_indices)))\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "    train_loader = DataLoader(dataset, batch_size=BATCH_SIZE,\n",
    "                              sampler=train_sampler)\n",
    "    validation_loader = DataLoader(dataset, batch_size=BATCH_SIZE,\n",
    "                                   sampler=valid_sampler)\n",
    "    return train_loader, validation_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def fit(model: SentenceTransformerAndClassifier, tepoch, epoch: int, loss_fn):\n",
    "    total_epoch_loss = 0\n",
    "    total_correct_classified_samples = 0\n",
    "    total_processed_samples = 0\n",
    "\n",
    "    model.train()\n",
    "    for step, data in enumerate(tepoch):\n",
    "        tepoch.set_description(f\"Epoch {epoch + 1}\")\n",
    "\n",
    "        input_ids = data[\"batch_encoding\"][\"input_ids\"].to(device)\n",
    "        attention_mask = data[\"batch_encoding\"][\"attention_mask\"].to(device)\n",
    "        input_ids = torch.squeeze(input_ids)\n",
    "        attention_mask = torch.squeeze(attention_mask)\n",
    "        targets = data[\"class_label\"].to(device)\n",
    "        actual_batch_size = len(targets)  # last batch can be smaller than BATCH_SIZE\n",
    "\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        mean_batch_loss = loss_fn(outputs, targets)  # CrossEntropy Loss is already averaged over the batch\n",
    "        total_batch_loss = mean_batch_loss.item() * actual_batch_size\n",
    "        total_epoch_loss += total_batch_loss\n",
    "\n",
    "        correct_classified_samples = torch.sum(predictions == targets).item()\n",
    "        batch_accuracy = correct_classified_samples / actual_batch_size\n",
    "        total_correct_classified_samples += correct_classified_samples\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        mean_batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_processed_samples += actual_batch_size\n",
    "        tepoch.set_postfix(loss=mean_batch_loss.item(), accuracy=batch_accuracy)\n",
    "\n",
    "    mean_epoch_loss = total_epoch_loss / total_processed_samples\n",
    "    epoch_accuracy = total_correct_classified_samples / total_processed_samples\n",
    "    return {\n",
    "        \"loss\": mean_epoch_loss,\n",
    "        \"acc\": epoch_accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def validate(model: SentenceTransformerAndClassifier, validation_loader: DataLoader, loss_fn):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_val_loss = 0\n",
    "        total_correct_classified_samples = 0\n",
    "        total_processed_samples = 0\n",
    "\n",
    "        for step, data in enumerate(validation_loader):\n",
    "            input_ids = data[\"batch_encoding\"][\"input_ids\"].to(device)\n",
    "            attention_mask = data[\"batch_encoding\"][\"attention_mask\"].to(device)\n",
    "            input_ids = torch.squeeze(input_ids)\n",
    "            attention_mask = torch.squeeze(attention_mask)\n",
    "            targets = data[\"class_label\"].to(device)\n",
    "            actual_batch_size = len(targets)  # last batch can be smaller than BATCH_SIZE\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            mean_batch_loss = loss_fn(outputs, targets)\n",
    "            total_batch_loss = mean_batch_loss.item() * actual_batch_size\n",
    "            total_val_loss += total_batch_loss\n",
    "\n",
    "            correct_classified_samples = torch.sum(predictions == targets).item()\n",
    "            batch_accuracy = correct_classified_samples / actual_batch_size\n",
    "            total_correct_classified_samples += correct_classified_samples\n",
    "\n",
    "            total_processed_samples += actual_batch_size\n",
    "\n",
    "        mean_val_loss = total_val_loss / total_processed_samples\n",
    "        val_accuracy = total_correct_classified_samples / total_processed_samples\n",
    "    return {\n",
    "        \"val_loss\": mean_val_loss,\n",
    "        \"val_acc\": val_accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+------------+\n",
      "|       Modules       | Parameters |\n",
      "+---------------------+------------+\n",
      "| linear_layer.weight |   393216   |\n",
      "|  linear_layer.bias  |    512     |\n",
      "|  classifier.weight  |    2560    |\n",
      "|   classifier.bias   |     5      |\n",
      "+---------------------+------------+\n",
      "Total trainable parameters: 396293\n"
     ]
    }
   ],
   "source": [
    "base_model = \"sentence-transformers/paraphrase-mpnet-base-v2\"\n",
    "model = SentenceTransformerAndClassifier(base_model, n_classes=5)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "model.to(device)\n",
    "model.describe_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  7556\n",
      "{0: 'address', 1: 'company_name', 2: 'location', 3: 'physical_good', 4: 'serial_number'}\n",
      "Train size: 7179, Val size: 377\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = ClassificationDataset(Path.joinpath(PROJECT_ROOT, \"data/processed\"), tokenizer, MAX_LEN)\n",
    "train_loader, validation_loader = prepare_dataloaders(dataset, VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Output()",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8eefff5b0a014bd29f3b239e766c99d1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display\n",
    "from ipywidgets import Output\n",
    "\n",
    "GRAPHS = Output()\n",
    "display(GRAPHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 15/15 [01:29<00:00,  5.98s/batch, accuracy=1, loss=0.419]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.089, Acc: 0.848\n",
      "Val Loss: 0.350, Val Acc: 0.981\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 15/15 [01:28<00:00,  5.89s/batch, accuracy=0.818, loss=0.452]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.278, Acc: 0.958\n",
      "Val Loss: 0.074, Val Acc: 0.997\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 15/15 [01:28<00:00,  5.88s/batch, accuracy=1, loss=0.0205]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.124, Acc: 0.970\n",
      "Val Loss: 0.037, Val Acc: 0.997\n",
      "\n"
     ]
    }
   ],
   "source": [
    "liveloss = PlotLosses()\n",
    "for epoch in range(EPOCHS):\n",
    "    with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "        train_logs = fit(model, tepoch, epoch, criterion)\n",
    "    print(\"Loss: {:.3f}, Acc: {:.3f}\".format(\n",
    "        train_logs[\"loss\"],\n",
    "        train_logs[\"acc\"],\n",
    "    ))\n",
    "\n",
    "    val_logs = validate(model, validation_loader, criterion)\n",
    "    print(\"Val Loss: {:.3f}, Val Acc: {:.3f}\".format(\n",
    "        val_logs[\"val_loss\"],\n",
    "        val_logs[\"val_acc\"],\n",
    "    ))\n",
    "    print()\n",
    "\n",
    "    logs = {**train_logs, **val_logs}\n",
    "    with GRAPHS:\n",
    "        liveloss.update(logs)\n",
    "        liveloss.send()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), Path.joinpath(PROJECT_ROOT, \"save_dict_model.pt\").absolute())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}